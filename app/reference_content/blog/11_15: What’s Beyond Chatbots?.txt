11/15: What’s Beyond Chatbots?

Image generated by DALL-E

I’ve been listening to how business leaders are discussing AI integration (Salesforce, Shopify and Typeface to name a few) and a major topic of conversation is ‘time-to-value’. For example, pre-GenAI, creating a data visualization of sales leads took an analyst 1+ hours but now takes 20min with GenAI to get a similar, if not better, output. But what can be done to reduce that 20min? Surprisingly, most business leaders don’t have a vision to answer this question, so we take matters into our own hands.

We could look to the model but reality is that most models are capable enough as-is. IMO UX/UI is the best next frontier to reduce time-to-value. The incumbent experience is chatbots and they are so popular now, in early days of the AI revolution, because…

They allow for users to easily onboard the model. UX is a dialogue (input-output) that comes naturally to humans.

(More of a 1b) They allow skeptical users to easily pressure test the model’s capabilities.

AI devs initially focused their time on the backend (embeddings, retrieval, vectorstore) before model providers started these tools out of the box.

AI devs’ creativity is pigeonholed by ChatGPT’s example.

But chat, in its exclusively dialogue form, is inefficient relative to GenAI’s capabilities. Chat works such that the better the query, and or conversation, the better the guiding context and the better the output. Users have to spoon-feed the initial ask AND the guiding context to the model, thus requiring higher cognitive load AND time-to-value, both of which are negative in their own right to the end consumer.

UX/UI Frameworks

Below are 4 Scenarios that sit within our primary ‘intent -> action’ framework for how we can think high-level about UX/UI with GenAI.

get intent -> do action (simple)

get intent -> do action (complex)

predict intent -> do action (simple)

predict intent -> do action (complex)

*intent = initial ask + guiding context

We can innovate along all axes (‘initial ask’, ‘guiding context’, ‘get vs. predict intent’, ‘simple vs. complex action’) of the framework with the goal of minimizing cognitive load ~ time-to-value AND/OR maximizing value of output. If we isolate the goal of minimizing cognitive load ~ time-to-value within the framework, the levers we have to pull are ‘get vs. predict intent’, ‘initial ask’ and ‘guiding context’. If we isolate the goal of maximizing value within the framework, the levers we have to pull are ‘simple vs. complex action’.

Quick aside, I postulate that ‘intent’ potentially defines complexity of ‘action’. If ‘full intent’ is known then ‘complex action’ is just a string of ‘simple actions’. However, we can’t convey our ‘full intent’ with 1 prompt. The human must be in the loop throughout the ‘complex action’ process. If this is true then we can collapse our 4 Scenarios into just 2 (‘get vs. predict intent’ -> ‘do action’). The only solve for not having the human-in-the-loop is AGI with its own ‘intent’. Because I believe ‘intent is all you need’, let’s just focus on that side of the framework and assume max value through ‘complex actions’ is realizable as an inevitable downstream outcome.

So let’s look at an ‘intent’-specific, secondary framework:



Red is base case and how most chatbots operate today. I use ChatGPT and prompt "I am writing a story on chatbots. Here is what I have so far: XXX" (‘guiding context’) + "Is it any good?" (‘initial ask’). Green is best case. There is no realistic 2nd middle case where ‘initial ask’ = ‘inferred’ and ‘guiding context’ = ‘human’ because the LLM can’t infer without pre-existing knowledge. Let’s look at some examples of yellow and green.

Yellow Example: OpenAI stateful API is an incremental win in automatically providing more guiding context to the chat conversation, such that there is a dialogue with appropriate memory. Although not UX/UI.

Green Example: Github Co-Pilot is the gold standard for UX/UI innovation. Enough ‘guiding context’ (the codebase) is ‘known’ such that my ‘asks’ can be ‘inferred’ and ultimately natively integrated into my existing workflow. I don’t have to go back and forth with a chatbot. If we think back to our primary framework, this is a perfect example where exhaustive intent prediction cascades to complex actions, such as writing a relevant function in my Python program. And if that’s not enough, Co-Pilot also gives multiple options at each inference step such that the human is kept in the loop with point and click UX.

My Vision

I’m not surprised Github Co-Pilot was the first to breakthrough the UX/UI wall because the model ‘understands’ code inherently, so there is no abstraction layer between the context and output. However, my vision is that all digital tools will implement similar co-pilots by giving models access to (1) users’ app usage and (2) the app’s codebase, to provide exhaustive ‘guiding context’ that ultimately enables ‘inference’ of ‘intial ask’. #2 will raise eyebrows arround security and privacy but again OpenAI has guaranteed no training on data fed to API and opensource is always an option.

But we can go a step farther than Github’s Co-pilot experience with multi-step inference, again with human in the loop. Then provide rationale, in natural language, for each option and inference path. We could even get cheeky with something like custom system instructions per user, such that I as a Salesforce user can give the LLM advanced notice that I am a novel user for example.

So what’s beyond chatbots? Native-integration that is specific to the application’s and user’s workflow.

Sources