12/15: From RLHF to Reinforcement Learning with AI Feedback

I recently wrote a post about RLHF but OpenAI dropped word today that they are working on Reinforcement Learning with AI Feedback (RLAIF). Reason being that more advanced AI systems may return outputs that would be inefficient or ineffective for humans to evaluate. For example, if an AI outputs thousands of lines of code, getting a human to evaluate this output would be incredibly tedious and/or potentially impossible from a technical perspective.

OpenAI’s approach is weak-to-strong generalization, can small models supervise large models? This is seemingly counter-intuitive because the large/strong model would only inherit the capability of the small/weak model. However, let’s assume the ‘weaker’ model is not objectively weak, just weaker than the stronger model. And the stronger model is strong in terms of its potential and capabilities, not necessarily stronger out of the box. As an analogy, let’s think about a football team positions coach who played in their younger years but were never the best player (weak model); then a current player joins the team who is obviously younger than the coach but also more athletic etc (strong model). That coach can train the young star more effectively and efficiently than a well-versed football fan (human supervisor). After years go by, that young star will be a better player than the positions coach ever was and can become a coach himself for the next generation, a generation that theoretically has more athletic potential than the once-was football star. The ultimate result is better football being played over time that the football fan can enjoy.

Watch Outs

As-is, RLAIF prioritizes performance over safety. By nature, the human is more out of the loop when we use models to fine-tune models. So what checks and balances can we put in place to keep the AI human aligned? Higher explainability? Explainability is a hot topic for deep learning methods in that it is difficult to determine the backend mechanics (parameters) that influence an output. However, do we need that level of detail when evaluating for alignment or can we just have the weak supervisor explain their evaluation in simpler human terms then have a human evaluate those condensed reports?